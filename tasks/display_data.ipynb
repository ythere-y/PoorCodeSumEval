{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo list:\n",
    "\n",
    "- [ ] python\n",
    "    - [ ] comment ratio\n",
    "    - [ ] identifier ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  os\n",
    "import  json\n",
    "from path_utils import FinalPath\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "TEST_MODEL = \"codellama/CodeLlama-7b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEST_MODEL)\n",
    "def get_char_num_per_line(code_data):\n",
    "    result =[]\n",
    "    for code in code_data:\n",
    "        print(code)\n",
    "        code_lines = code.split(\"\\n\")\n",
    "        char_num_per_line = 0\n",
    "        line_counter = 0\n",
    "        for line in code_lines:\n",
    "            cur_char_num = len(line)\n",
    "            print(f'{cur_char_num} -> {line}')\n",
    "            if cur_char_num <= 2:\n",
    "                continue\n",
    "            char_num_per_line += cur_char_num\n",
    "            line_counter += 1\n",
    "        char_num_per_line = char_num_per_line / line_counter\n",
    "        result.append(char_num_per_line)\n",
    "        break\n",
    "    return result\n",
    "def get_code_token_num_per_line(code_data):\n",
    "    result = []\n",
    "    for code in code_data:\n",
    "        code_lines = code.split(\"\\n\")\n",
    "        token_num_per_line = 0\n",
    "        line_counter = 0\n",
    "        for line in code_lines:\n",
    "            cur_token_num = len(tokenizer.tokenize(line))\n",
    "            if cur_token_num <= 2:\n",
    "                continue\n",
    "            token_num_per_line += cur_token_num\n",
    "            line_counter += 1\n",
    "        token_num_per_line = token_num_per_line / line_counter\n",
    "        result.append(token_num_per_line)\n",
    "    return result\n",
    "def handle_java_gen(generations):\n",
    "    new_generations = [\n",
    "        gen.strip()\n",
    "        .replace(\"The goal of this function is to \", \"\")\n",
    "        .strip()\n",
    "        .strip('\"')\n",
    "        .strip(\"'\")\n",
    "        for gen in generations\n",
    "    ]\n",
    "    return new_generations\n",
    "\n",
    "def load_local_datasetv2(type_name, partition_name,mode_name, lang_name):\n",
    "    file_path = f\"../local_data/second/{type_name}/{partition_name}/{mode_name}/{lang_name}/CSN\"\n",
    "    dataset = load_from_disk(file_path)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bind Code, ref, gen, score (all origin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "language = \"python\"\n",
    "dataset = load_local_datasetv2(\"origin\", \"origin\", \"origin\", language)\n",
    "\n",
    "# gen_path,ref_path = FinalPath.get_generation_path(\"CodeLlama-7b-hf\",\"origin\",'origin','origin','java',task_name=\"work\",start_point=0,limit=2000)\n",
    "gen_path = (\n",
    "    f\"../ref_and_gen/CodeLlama-7b-hf/origin/origin/origin/{language}/CSN/work_gen_[0-2000].json\"\n",
    ")\n",
    "ref_path = (\n",
    "    f\"../ref_and_gen/CodeLlama-7b-hf/origin/origin/origin/{language}/CSN/work_ref_[0-2000].json\"\n",
    ")\n",
    "with open(ref_path, \"r\") as f:\n",
    "    refs = json.load(f)\n",
    "with open(gen_path,'r')as f:\n",
    "    gens = json.load(f)\n",
    "\n",
    "gens = handle_java_gen(gens)\n",
    "\n",
    "score_bert_path = \"../scores/results/CodeLlama-7b-hf/work_origin_origin_2000_BERTScore.json\"\n",
    "score_bleu_path = \"../scores/results/CodeLlama-7b-hf/work_origin_origin_2000_BLEUScore.json\"\n",
    "\n",
    "with open(score_bert_path, \"r\") as f:\n",
    "    score_bert = json.load(f)\n",
    "with open(score_bleu_path, \"r\") as f:\n",
    "    score_bleu = json.load(f)\n",
    "    \n",
    "print(len(gens))\n",
    "print(len(refs))\n",
    "print(len(score_bert))\n",
    "print(len(score_bleu))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare code token per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "prompt_list = []\n",
    "for idx in range(2000):\n",
    "    sample = dataset[idx]\n",
    "    code_string = sample[\"code\"]\n",
    "    doc_string = sample[\"docstring\"]\n",
    "    \n",
    "    prompt = code_string.replace(doc_string, '',1)\n",
    "    prompt_list.append(prompt)\n",
    "print(len(prompt_list))\n",
    "\n",
    "token_num_per_line = get_code_token_num_per_line(prompt_list)\n",
    "print(len(token_num_per_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protected final void fastPathOrderedEmit(U value, boolean delayError, Disposable disposable) {\n",
      "        final Observer<? super V> observer = downstream;\n",
      "        final SimplePlainQueue<U> q = queue;\n",
      "\n",
      "        if (wip.get() == 0 && wip.compareAndSet(0, 1)) {\n",
      "            if (q.isEmpty()) {\n",
      "                accept(observer, value);\n",
      "                if (leave(-1) == 0) {\n",
      "                    return;\n",
      "                }\n",
      "            } else {\n",
      "                q.offer(value);\n",
      "            }\n",
      "        } else {\n",
      "            q.offer(value);\n",
      "            if (!enter()) {\n",
      "                return;\n",
      "            }\n",
      "        }\n",
      "        QueueDrainHelper.drainLoop(q, observer, delayError, disposable, this);\n",
      "    }\n",
      "94 -> protected final void fastPathOrderedEmit(U value, boolean delayError, Disposable disposable) {\n",
      "56 ->         final Observer<? super V> observer = downstream;\n",
      "44 ->         final SimplePlainQueue<U> q = queue;\n",
      "0 -> \n",
      "56 ->         if (wip.get() == 0 && wip.compareAndSet(0, 1)) {\n",
      "30 ->             if (q.isEmpty()) {\n",
      "40 ->                 accept(observer, value);\n",
      "37 ->                 if (leave(-1) == 0) {\n",
      "27 ->                     return;\n",
      "17 ->                 }\n",
      "20 ->             } else {\n",
      "31 ->                 q.offer(value);\n",
      "13 ->             }\n",
      "16 ->         } else {\n",
      "27 ->             q.offer(value);\n",
      "27 ->             if (!enter()) {\n",
      "23 ->                 return;\n",
      "13 ->             }\n",
      "9 ->         }\n",
      "78 ->         QueueDrainHelper.drainLoop(q, observer, delayError, disposable, this);\n",
      "5 ->     }\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[33.15]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_char_num_per_line(prompt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conbine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bind_data = []\n",
    "for i in range(2000):\n",
    "    cur_data = {\n",
    "        \"prompt\":prompt_list[i],\n",
    "        \"gen\":gens[i],\n",
    "        \"ref\":refs[i],\n",
    "        \"score_bert\":score_bert[i],\n",
    "        \"score_bleu\":score_bleu[i],\n",
    "        \"token_num_per_line\":token_num_per_line[i],\n",
    "        \n",
    "    }\n",
    "    # print(json.dumps(cur_data,indent=4))\n",
    "    bind_data.append(cur_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check Data cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrcoef_bert:[[1.         0.00391361]\n",
      " [0.00391361 1.        ]]\n",
      "corrcoef_bleu:[[ 1.        -0.0031857]\n",
      " [-0.0031857  1.       ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "bert_score = [data[\"score_bert\"] for data in bind_data]\n",
    "bleu_score = [data[\"score_bleu\"] for data in bind_data]\n",
    "token_num_per_line = [data[\"token_num_per_line\"] for data in bind_data]\n",
    "\n",
    "# use corrcoef \n",
    "corrcoef_bert = np.corrcoef(bert_score, token_num_per_line)\n",
    "corrcoef_bleu = np.corrcoef(bleu_score, token_num_per_line)\n",
    "print(f'corrcoef_bert:{corrcoef_bert}')\n",
    "print(f'corrcoef_bleu:{corrcoef_bleu}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
